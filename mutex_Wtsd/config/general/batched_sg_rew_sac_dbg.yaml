general:
  algorithm: batched_sg_rew_sac #Algorithm used for training
  master_port: !!str "12353" #port num on localhost for icp
  cross_validate_hp: false # make gridsearch cv of hp
  no_save: false #dont save models
  test_score_only: false #no learning only validation
  num_processes: 1 #Number of training async agents
  n_gpu_per_proc: 1 #Number of gpus per process
  evaluate: false #evaluate existing model
  seed: 123 #Random seed
  target_dir: test_dbg1 #Save folder
  base_dir: /g/kreshuk/hilt/projects/fewShotLearning/mutexWtsd #Save folder
  ## env and model defs
  model_name: ''

  #Pretrained model (state dict)
  model_fe_name: agent_model_fe_extr
  model_name_dest: agent_model #(state dict) is safed to
  n_edge_features: 10 #number of initial edge features
  n_actions: 3 #number of actions on edge
  ## feature extractor
  weight_edge_loss: 0.3
  weight_side_loss: 10
  n_embedding_features: 16 #number of embedding feature channels
  n_raw_channels: 1 #number of channels in raw data
  fe_extr_warmup: false

  #pretrain the feature extractor with contrastive loss
  fe_warmup_iterations: 60 #number of iterations of feature extrqactor warmup
  fe_warmup_batch_size: 10 #batch size for feature extractor warmup
  no_fe_extr_optim: true #optimize feature extractor with ril loss
  ## main training (env, trainer)
  T_max: 10000 #Number of training steps
  t_max: 100 #Mem size
  max_episode_length: 15 #Maximum episode length
  eps_rule: self_reg_exp_avg #
  eps_final: 0.005 #
  eps_scaling: 1 #
  eps_offset: 0 #
  stop_qual_rule: constant # running_average #
  stop_qual_final: 5 #
  stop_qual_scaling: 40 #
  stop_qual_offset: 5 #
  stop_qual_ra_bw: 20 #running average bandwidth
  stop_qual_ra_off: -15 #running average offset
  reward_function: fully_supervised #Reward function
  action_agression: 0.1 #value by which one action changes state
  ## Training specifics (agent)
  discount: 0.5 #Discount factor
  max_gradient_norm: 100 #Gradient L2 norm clipping
  l2_reg_params_weight: 0.0 #Gradient L2 weight
  p_loss_weight: 1 # policy loss weight
  v_loss_weight: 1 # value loss weight
  s_subgraph: 10
  ## Optimization
  min_lr: 0.0 #min Learning rate
  lr: 0.00006 #
  Adam_weight_decay: 0 #
  Adam_betas: [0.9, 0.999] #Adam decay factors
  ## runtime ctl
  runtime_config:
    lr: 0.000006
    eps: 1 #epsilon for manual config during runtime
    safe_model: false #the model is saved
    add_noise: false #noise is added to the rewards
    actor_lr: 1.0e-4
    critic_lr: 1.0e-4
    alpha_lr: 6.0e-4

