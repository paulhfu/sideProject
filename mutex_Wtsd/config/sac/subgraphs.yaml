sac:
  # specific confics for the sac algorithm
  reward_function: sub_graph_dice #Reward function: sub_graph_dice, fully_supervised, defining_rules, defining_rules_lg,defining_rules_edge_based
  data_shape: [128, 128]
  s_subgraph: [10, 20, 40] # [8, 16, 32, 64] # subgraph sizes
  reg_scaler: 10
  discount: 0.99
  init_temperature: 0.1
  temperature_regulation: optimized #constant, follow_quality, optimized
  alpha_lr: 4e-3
  alpha_betas: [0.9, 0.999]
  actor_lr: 2e-4
  actor_betas: [0.9, 0.999]
  actor_update_frequency: 1
  critic_lr: 2e-4
  critic_betas: [0.9, 0.999]
  critic_tau: 0.005
  critic_target_update_frequency: 2
  sl_beta: 10
  weight_tolerance_attr: 0.1
  weight_tolerance_rep: 0.9
  MC_DQL: false # monte carlo learning for the critic
#  target_entropy: -100
  diag_gaussian_actor:
    log_std_bounds: [-5, 2]

