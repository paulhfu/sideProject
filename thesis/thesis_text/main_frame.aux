\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\babel@aux{USenglish}{}
\citation{SB_all}
\citation{wolf2019mutex}
\citation{brab2017semantic}
\citation{Schroff_2015}
\newlabel{sec:ref}{{}{5}{}{section*.1}{}}
\@writefile{toc}{\contentsline {section}{Summary of Notation}{5}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Introduction}{6}{section.0.1}}
\newlabel{sec:introduction}{{0.1}{6}{Introduction}{section.0.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Motivation}{7}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:rl_for_seg}{{1}{7}{Motivation}{chapter.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Preliminaries}{8}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{sec:preliminaries}{{2}{8}{Preliminaries}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Image segmentation}{8}{section.2.1}}
\newlabel{sec:prel_imagesegmentation}{{2.1}{8}{Image segmentation}{section.2.1}{}}
\citation{SB_all}
\citation{SB_all}
\citation{SB_all}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Reinforcement Learning (RL)}{9}{section.2.2}}
\newlabel{ssec:rl}{{2.2}{9}{Reinforcement Learning (RL)}{section.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces agent environment interaction \cite  {SB_all}\relax }}{10}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig_rl_gen}{{2.1}{10}{agent environment interaction \cite {SB_all}\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Value functions}{10}{subsection.2.2.1}}
\citation{SBQL}
\citation{SBeligibility}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Q-learning}{11}{subsection.2.2.2}}
\citation{liu2018breaking}
\citation{PGBS}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Policy gradient methods}{12}{subsection.2.2.3}}
\citation{AAAIziebert}
\citation{DBLP:journals/corr/HaarnojaTAL17}
\citation{DBLP:journals/corr/abs-1906-02771}
\citation{papamakarios2019normalizing}
\citation{haarnoja2018soft}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Maximum Entropy Reinforcement Learning}{13}{subsection.2.2.4}}
\citation{haarnoja2018soft}
\citation{kingma2013autoencoding}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Soft Actor-Critic (SAC)}{14}{subsection.2.2.5}}
\newlabel{ssec:sac}{{2.2.5}{14}{Soft Actor-Critic (SAC)}{subsection.2.2.5}{}}
\citation{haarnoja2018soft}
\citation{hessel2017rainbow}
\citation{DBLP:journals/corr/HasseltGS15}
\citation{DBLP:journals/corr/HasseltGS15}
\citation{schaul2015prioritized}
\citation{SBQL}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Common optimization methods}{15}{subsection.2.2.6}}
\newlabel{ssec:common_opt}{{2.2.6}{15}{Common optimization methods}{subsection.2.2.6}{}}
\newlabel{text:doublQ}{{2.2.6}{15}{Common optimization methods}{subsection.2.2.6}{}}
\citation{SBQL}
\citation{DBLP:journals/corr/WangFL15}
\citation{bruna2013spectral}
\citation{Bronstein_2017}
\citation{gilmer2017neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.7}Reparameterization}{16}{subsection.2.2.7}}
\newlabel{ssec:reparam}{{2.2.7}{16}{Reparameterization}{subsection.2.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.8}Normalizing flows}{16}{subsection.2.2.8}}
\newlabel{ssec:norm_flows}{{2.2.8}{16}{Normalizing flows}{subsection.2.2.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Geometric deep learning}{16}{section.2.3}}
\newlabel{sec:gcn}{{2.3}{16}{Geometric deep learning}{section.2.3}{}}
\citation{wolf2019mutex}
\citation{wolf2019mutex}
\citation{wolf2019mutex}
\citation{wolf2019mutex}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Mutex watershed}{17}{section.2.4}}
\newlabel{sec:mtx_wtsd}{{2.4}{17}{Mutex watershed}{section.2.4}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Mutex Watershed \cite  {wolf2019mutex}\relax }}{17}{algocf.1}}
\newlabel{algo:mtx_wtsd}{{1}{17}{Mutex watershed}{algocf.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Some iterations of algorithm \ref  {algo:mtx_wtsd} applied to a graph with weighted attractive edges (green) and repulsive (ref) edges. Edges that are part of the active set $A$ at each iteration are shown in bold. On termination (f), the connected components in $A \cap E^+$ represent the partitions of the final partitioning. Edges that are not added to $A$ because of the violation of $\mathcal  {C}_0$ or $\mathcal  {C}_1$ are highlighted in blue and yellow respectively \cite  {wolf2019mutex}\relax }}{18}{figure.caption.3}}
\newlabel{fig_mtxwtsd1}{{2.2}{18}{Some iterations of algorithm \ref {algo:mtx_wtsd} applied to a graph with weighted attractive edges (green) and repulsive (ref) edges. Edges that are part of the active set $A$ at each iteration are shown in bold. On termination (f), the connected components in $A \cap E^+$ represent the partitions of the final partitioning. Edges that are not added to $A$ because of the violation of $\mathcal {C}_0$ or $\mathcal {C}_1$ are highlighted in blue and yellow respectively \cite {wolf2019mutex}\relax }{figure.caption.3}{}}
\citation{wolf2019mutex}
\citation{wolf2019mutex}
\citation{10.1007/978-3-642-23094-3_3}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Image partitioning by multicuts}{19}{section.2.5}}
\newlabel{sec:multicut}{{2.5}{19}{Image partitioning by multicuts}{section.2.5}{}}
\citation{10.1007/978-3-642-23094-3_3}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Principal component analysis}{20}{section.2.6}}
\newlabel{sec:pca}{{2.6}{20}{Principal component analysis}{section.2.6}{}}
\citation{brab2017semantic}
\citation{brab2017semantic}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Loss functions}{21}{section.2.7}}
\newlabel{ssec:losses}{{2.7}{21}{Loss functions}{section.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}Dice loss}{21}{subsection.2.7.1}}
\newlabel{ssec:loss_dice}{{2.7.1}{21}{Dice loss}{subsection.2.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}Contrasive loss}{21}{subsection.2.7.2}}
\newlabel{ssec:loss_contrastive}{{2.7.2}{21}{Contrasive loss}{subsection.2.7.2}{}}
\citation{brab2017semantic}
\citation{brab2017semantic}
\citation{Schroff_2015}
\citation{Schroff_2015}
\citation{Schroff_2015}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces As defined by the loss, this are the hinged inter-pulling and intra-pushing forces acting on points in the embedding space \cite  {brab2017semantic}\relax }}{23}{figure.caption.4}}
\newlabel{fig_contrastive}{{2.3}{23}{As defined by the loss, this are the hinged inter-pulling and intra-pushing forces acting on points in the embedding space \cite {brab2017semantic}\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.3}Triplet loss}{23}{subsection.2.7.3}}
\newlabel{ssec:loss_triplet}{{2.7.3}{23}{Triplet loss}{subsection.2.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Position of triplets in the embedding space before and after training \cite  {Schroff_2015}\relax }}{23}{figure.caption.5}}
\newlabel{fig_triplet}{{2.4}{23}{Position of triplets in the embedding space before and after training \cite {Schroff_2015}\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methods}{25}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:rl_for_seg}{{3}{25}{Methods}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Using RL for the image segmentation task}{25}{section.3.1}}
\newlabel{sec:rl_for_seg}{{3.1}{25}{Using RL for the image segmentation task}{section.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Using RL for predicting affinities}{25}{section.3.2}}
\newlabel{sec:rl_for_seg}{{3.2}{25}{Using RL for predicting affinities}{section.3.2}{}}
\citation{10.1007/978-3-642-23094-3_3}
\citation{monti2016geometric}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Overview over the pipeline}{26}{section.3.3}}
\newlabel{seg:ov_pip}{{3.3}{26}{Overview over the pipeline}{section.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Rough sketch of the proposed pipeline. Starting from raw data (top left), a superpixel graph is obtained with mutex watershed \ref  {algo:mtx_wtsd} and pixel embeddings (top right) with an embedding network. Computing node features based on the average pixel embedding per superpixel a GCNN predicts logits on the edges of the superpixel graph. The logits are used to compute chances which in turn are used to compute costs based on which a multicut of the superpixel graph is computed and a segmentation is obtained from the multicut and the region adjacency graph of the superpixels. This segmentation is then evaluated and a reward is produced which is then used in the RL loss.\relax }}{27}{figure.caption.6}}
\newlabel{overview}{{3.1}{27}{Rough sketch of the proposed pipeline. Starting from raw data (top left), a superpixel graph is obtained with mutex watershed \ref {algo:mtx_wtsd} and pixel embeddings (top right) with an embedding network. Computing node features based on the average pixel embedding per superpixel a GCNN predicts logits on the edges of the superpixel graph. The logits are used to compute chances which in turn are used to compute costs based on which a multicut of the superpixel graph is computed and a segmentation is obtained from the multicut and the region adjacency graph of the superpixels. This segmentation is then evaluated and a reward is produced which is then used in the RL loss.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}The pipeline in the RL terminology}{28}{section.3.4}}
\newlabel{seg:pip_tl_term}{{3.4}{28}{The pipeline in the RL terminology}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}The state}{28}{subsection.3.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}The actions}{28}{subsection.3.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}The reward}{28}{subsection.3.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}The agent}{29}{subsection.3.4.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.5}The environment}{29}{subsection.3.4.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.6}The problem of local optima}{29}{subsection.3.4.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A rough sketch of the reward calcuation on subgraphs with 7 edges and the resulting losses in an actor critic setting\relax }}{30}{figure.caption.7}}
\newlabel{overview}{{3.2}{30}{A rough sketch of the reward calcuation on subgraphs with 7 edges and the resulting losses in an actor critic setting\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.7}Definition of the RL algorithm}{30}{subsection.3.4.7}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Obtaining superpixels from mutex watershed}{31}{section.3.5}}
\newlabel{seg:pip_mutex}{{3.5}{31}{Obtaining superpixels from mutex watershed}{section.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}The embedding network}{31}{section.3.6}}
\newlabel{seg:pip_embed}{{3.6}{31}{The embedding network}{section.3.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}The actor critic networks}{32}{section.3.7}}
\newlabel{seg:sag_gcn}{{3.7}{32}{The actor critic networks}{section.3.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Finding subgraphs}{34}{section.3.8}}
\newlabel{seg:sag_gcn}{{3.8}{34}{Finding subgraphs}{section.3.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Technical details}{34}{section.3.9}}
\newlabel{chap:tech}{{3.9}{34}{Technical details}{section.3.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.1}Batch processing}{34}{subsection.3.9.1}}
\newlabel{sec:batchp}{{3.9.1}{34}{Batch processing}{subsection.3.9.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Technical realization}{35}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:impl}{{4}{35}{Technical realization}{chapter.4}{}}
\bibstyle{IEEEtran}
\bibdata{bibliography.bib}
\newlabel{sec:ref}{{4}{36}{Technical realization}{section*.8}{}}
\@writefile{toc}{\contentsline {section}{References}{36}{section*.8}}
\bibcite{SB_all}{1}
\bibcite{SBQL}{2}
\bibcite{SBeligibility}{3}
\bibcite{liu2018breaking}{4}
\bibcite{PGBS}{5}
\bibcite{AAAIziebert}{6}
\bibcite{DBLP:journals/corr/HaarnojaTAL17}{7}
\bibcite{DBLP:journals/corr/abs-1906-02771}{8}
\bibcite{papamakarios2019normalizing}{9}
\bibcite{haarnoja2018soft}{10}
\bibcite{kingma2013autoencoding}{11}
\bibcite{hessel2017rainbow}{12}
\bibcite{DBLP:journals/corr/HasseltGS15}{13}
\bibcite{schaul2015prioritized}{14}
\bibcite{DBLP:journals/corr/WangFL15}{15}
\bibcite{bruna2013spectral}{16}
\bibcite{Bronstein_2017}{17}
\bibcite{gilmer2017neural}{18}
\bibcite{wolf2019mutex}{19}
\bibcite{10.1007/978-3-642-23094-3_3}{20}
\bibcite{brab2017semantic}{21}
\bibcite{Schroff_2015}{22}
\bibcite{monti2016geometric}{23}
