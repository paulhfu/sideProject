\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\babel@aux{USenglish}{}
\newlabel{sec:ref}{{}{2}{}{section*.1}{}}
\newlabel{sec:ref}{{}{2}{}{section*.2}{}}
\citation{wang2016sample}
\citation{haarnoja2018soft}
\citation{wang2016sample}
\citation{haarnoja2018soft}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{6}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Motivation}{6}{section.1.1}}
\newlabel{chap:motivation}{{1.1}{6}{Motivation}{section.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Contribution}{6}{section.1.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Preliminaries}{7}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{sec:preliminaries}{{2}{7}{Preliminaries}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Image segmentation}{7}{section.2.1}}
\newlabel{sec:prel_imagesegmentation}{{2.1}{7}{Image segmentation}{section.2.1}{}}
\citation{SB_all}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Reinforcement learning (RL)}{8}{section.2.2}}
\newlabel{ssec:rl}{{2.2}{8}{Reinforcement learning (RL)}{section.2.2}{}}
\citation{SB_all}
\citation{SB_all}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces agent environment interaction \cite  {SB_all}\relax }}{9}{figure.caption.3}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig_rl_gen}{{2.1}{9}{agent environment interaction \cite {SB_all}\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Value functions}{10}{subsection.2.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Q-learning}{10}{subsection.2.2.2}}
\citation{SBQL}
\citation{SBeligibility}
\citation{liu2018breaking}
\citation{PGBS}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Policy gradient methods}{11}{subsection.2.2.3}}
\citation{AAAIziebert}
\citation{DBLP:journals/corr/HaarnojaTAL17}
\citation{DBLP:journals/corr/abs-1906-02771}
\citation{papamakarios2019normalizing}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Maximum Entropy Reinforcement Learning}{12}{subsection.2.2.4}}
\citation{haarnoja2018soft}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Soft Actor-Critic (SAC)}{13}{subsection.2.2.5}}
\newlabel{ssec:sac}{{2.2.5}{13}{Soft Actor-Critic (SAC)}{subsection.2.2.5}{}}
\citation{haarnoja2018soft}
\citation{haarnoja2018soft}
\citation{hessel2017rainbow}
\citation{DBLP:journals/corr/HasseltGS15}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Common optimization methods}{14}{subsection.2.2.6}}
\newlabel{ssec:common_opt}{{2.2.6}{14}{Common optimization methods}{subsection.2.2.6}{}}
\newlabel{text:doublQ}{{2.2.6}{14}{Common optimization methods}{subsection.2.2.6}{}}
\citation{schaul2015prioritized}
\citation{SBQL}
\citation{DBLP:journals/corr/WangFL15}
\citation{mnih2016asynchronous}
\citation{bruna2013spectral}
\citation{Bronstein_2017}
\citation{gilmer2017neural}
\citation{wolf2019mutex}
\newlabel{ssec::a3c}{{2.2.6}{16}{Common optimization methods}{equation.2.2.31}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Geometric deep learning}{16}{section.2.3}}
\newlabel{sec:gcn}{{2.3}{16}{Geometric deep learning}{section.2.3}{}}
\citation{wolf2019mutex}
\citation{wolf2019mutex}
\citation{wolf2019mutex}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Mutex watershed}{17}{section.2.4}}
\newlabel{sec:mtx_wtsd}{{2.4}{17}{Mutex watershed}{section.2.4}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Mutex Watershed \cite  {wolf2019mutex}\relax }}{17}{algocf.1}}
\newlabel{algo:mtx_wtsd}{{1}{17}{Mutex watershed}{algocf.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces \cite  {wolf2019mutex} Some iterations of algorithm \ref  {algo:mtx_wtsd} applied to a graph with weighted attractive edges (green) and repulsive (red) edges. Edges that are part of the active set $A$ at each iteration are shown in bold. On termination (f), the connected components in $A \cap E^+$ represent the partitions of the final partitioning. Edges that are not added to $A$ because of the violation of $\mathcal  {C}_0$ or $\mathcal  {C}_1$ are highlighted in blue and yellow respectively.\relax }}{18}{figure.caption.4}}
\newlabel{fig_mtxwtsd1}{{2.2}{18}{\cite {wolf2019mutex} Some iterations of algorithm \ref {algo:mtx_wtsd} applied to a graph with weighted attractive edges (green) and repulsive (red) edges. Edges that are part of the active set $A$ at each iteration are shown in bold. On termination (f), the connected components in $A \cap E^+$ represent the partitions of the final partitioning. Edges that are not added to $A$ because of the violation of $\mathcal {C}_0$ or $\mathcal {C}_1$ are highlighted in blue and yellow respectively.\relax }{figure.caption.4}{}}
\citation{wolf2019mutex}
\citation{wolf2019mutex}
\citation{10.1007/978-3-642-23094-3_3}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Image partitioning by multicuts}{19}{section.2.5}}
\newlabel{sec:multicut}{{2.5}{19}{Image partitioning by multicuts}{section.2.5}{}}
\citation{10.1007/978-3-642-23094-3_3}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Principal component analysis}{20}{section.2.6}}
\newlabel{sec:pca}{{2.6}{20}{Principal component analysis}{section.2.6}{}}
\citation{Sudre_2017}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Loss functions}{21}{section.2.7}}
\newlabel{ssec:losses}{{2.7}{21}{Loss functions}{section.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}Generalized dice loss}{21}{subsection.2.7.1}}
\newlabel{ssec:loss_dice}{{2.7.1}{21}{Generalized dice loss}{subsection.2.7.1}{}}
\citation{brab2017semantic}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}Contrasive loss}{22}{subsection.2.7.2}}
\newlabel{ssec:loss_contrastive}{{2.7.2}{22}{Contrasive loss}{subsection.2.7.2}{}}
\citation{brab2017semantic}
\citation{brab2017semantic}
\citation{Schroff_2015}
\citation{Schroff_2015}
\citation{Schroff_2015}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces \cite  {brab2017semantic} As defined by the loss, this are the hinged inter pulling and intra pushing forces acting on points in the embedding space\relax }}{23}{figure.caption.5}}
\newlabel{fig_contrastive}{{2.3}{23}{\cite {brab2017semantic} As defined by the loss, this are the hinged inter pulling and intra pushing forces acting on points in the embedding space\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.3}Triplet loss}{23}{subsection.2.7.3}}
\newlabel{ssec:loss_triplet}{{2.7.3}{23}{Triplet loss}{subsection.2.7.3}{}}
\citation{kingma2013autoencoding}
\citation{10.1145/361237.361242}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces \cite  {Schroff_2015} Position of triplets in the embedding space before and after a optimization step\relax }}{24}{figure.caption.6}}
\newlabel{fig_triplet}{{2.4}{24}{\cite {Schroff_2015} Position of triplets in the embedding space before and after a optimization step\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}The reparameterization trick}{24}{section.2.8}}
\newlabel{ssec:reparam}{{2.8}{24}{The reparameterization trick}{section.2.8}{}}
\citation{10.1145/361237.361242}
\citation{10.1145/361237.361242}
\citation{10.1145/361237.361242}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}The Heugh transform}{25}{section.2.9}}
\newlabel{ssec:heugh_tf}{{2.9}{25}{The Heugh transform}{section.2.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces \cite  {10.1145/361237.361242} latent variables of a line\relax }}{25}{figure.caption.7}}
\newlabel{fig:norm_line}{{2.5}{25}{\cite {10.1145/361237.361242} latent variables of a line\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methods}{27}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:rl_for_seg}{{3}{27}{Methods}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Using RL for the image segmentation task}{27}{section.3.1}}
\newlabel{sec:rl_for_seg}{{3.1}{27}{Using RL for the image segmentation task}{section.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Using RL for pixel affinity predictions}{27}{section.3.2}}
\newlabel{sec:rl_for_seg}{{3.2}{27}{Using RL for pixel affinity predictions}{section.3.2}{}}
\citation{10.1007/978-3-642-23094-3_3}
\citation{monti2016geometric}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Overview over the proposed pipeline}{28}{section.3.3}}
\newlabel{seg:ov_pip}{{3.3}{28}{Overview over the proposed pipeline}{section.3.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}The pipeline in reinforcement learning terminology}{29}{section.3.4}}
\newlabel{seg:pip_tl_term}{{3.4}{29}{The pipeline in reinforcement learning terminology}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}The state}{29}{subsection.3.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}The actions}{29}{subsection.3.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces A rough sketch of the proposed pipeline. Starting from raw data (top left), a superpixel graph is obtained with the Mutex Watershed algorithm \ref  {algo:mtx_wtsd} and pixel embeddings (top right) are obtained with an embedding network. With node features computed as the average pixel embedding per superpixel a GCNN predicts logits on the edges of the superpixel graph. The logits are used to compute chances which in turn are used to compute costs based on which a min cost multicut of the superpixel graph is computed from which a segmentation is obtained. This segmentation is then evaluated and a reward is produced which is then used in the RL loss.\relax }}{30}{figure.caption.8}}
\newlabel{overview}{{3.1}{30}{A rough sketch of the proposed pipeline. Starting from raw data (top left), a superpixel graph is obtained with the Mutex Watershed algorithm \ref {algo:mtx_wtsd} and pixel embeddings (top right) are obtained with an embedding network. With node features computed as the average pixel embedding per superpixel a GCNN predicts logits on the edges of the superpixel graph. The logits are used to compute chances which in turn are used to compute costs based on which a min cost multicut of the superpixel graph is computed from which a segmentation is obtained. This segmentation is then evaluated and a reward is produced which is then used in the RL loss.\relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}The reward}{30}{subsection.3.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}The agent}{31}{subsection.3.4.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.5}The environment}{31}{subsection.3.4.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.6}The problem of local optima}{31}{subsection.3.4.6}}
\citation{papamakarios2019normalizing}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A rough sketch of the reward calcuation on subgraphs with 7 edges and the resulting losses in an actor critic setting\relax }}{32}{figure.caption.9}}
\newlabel{reward_calc}{{3.2}{32}{A rough sketch of the reward calcuation on subgraphs with 7 edges and the resulting losses in an actor critic setting\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.7}Definition of the RL algorithm}{32}{subsection.3.4.7}}
\citation{ronneberger2015unet}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Obtaining superpixels from mutex watershed}{33}{section.3.5}}
\newlabel{seg:pip_mutex}{{3.5}{33}{Obtaining superpixels from mutex watershed}{section.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}The embedding network}{33}{section.3.6}}
\newlabel{seg:pip_embed}{{3.6}{33}{The embedding network}{section.3.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}The actor critic networks}{34}{section.3.7}}
\newlabel{sec:sag_gcn}{{3.7}{34}{The actor critic networks}{section.3.7}{}}
\citation{densestSg}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Finding subgraphs}{37}{section.3.8}}
\newlabel{seg:sag_gcn}{{3.8}{37}{Finding subgraphs}{section.3.8}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Dense subgraphs in a rag\relax }}{38}{algocf.2}}
\newlabel{algo:sgs}{{2}{38}{Finding subgraphs}{algocf.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}Thoughts on dependence}{38}{subsection.3.8.1}}
\citation{Fey/Lenssen/2019}
\citation{ioffe2015batch}
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Technical details}{39}{section.3.9}}
\newlabel{sec:tech}{{3.9}{39}{Technical details}{section.3.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.1}Batch processing}{39}{subsection.3.9.1}}
\newlabel{ssec:batchp}{{3.9.1}{39}{Batch processing}{subsection.3.9.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiments and results}{40}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:exp_res}{{4}{40}{Experiments and results}{chapter.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Example raw image\relax }}{40}{figure.caption.10}}
\newlabel{fig:exmpl}{{4.1}{40}{Example raw image\relax }{figure.caption.10}{}}
\citation{kingma2014adam}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Values in first three principal components of pixel embeddings\relax }}{41}{figure.caption.11}}
\newlabel{fig:pixembeddings}{{4.2}{41}{Values in first three principal components of pixel embeddings\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.1}Supervised training}{41}{subsection.4.0.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Result using the dice reward after $80000$ optimization steps on mini batches of size $10$. The gt image was obtained by computing the multicut based on the ground truth edges\relax }}{42}{figure.caption.12}}
\newlabel{fig:resa_dice}{{4.3}{42}{Result using the dice reward after $80000$ optimization steps on mini batches of size $10$. The gt image was obtained by computing the multicut based on the ground truth edges\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Visualization of the values of the edge features is their first $3$ principal components (PCA1-3) and the ground truth edge values (GT) after training with the dice rewards for $80000$ optimization steps on mini batches of size $10$.\relax }}{42}{figure.caption.13}}
\newlabel{fig:res_edge_embed}{{4.4}{42}{Visualization of the values of the edge features is their first $3$ principal components (PCA1-3) and the ground truth edge values (GT) after training with the dice rewards for $80000$ optimization steps on mini batches of size $10$.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces History of predicted means for the policy (supervised reward setting)\relax }}{42}{figure.caption.14}}
\newlabel{fig:pred_means}{{4.5}{42}{History of predicted means for the policy (supervised reward setting)\relax }{figure.caption.14}{}}
\citation{400568}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.2}Unsupervised training}{43}{subsection.4.0.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Result using the unsupervised reward after $80000$ optimization steps on mini batches of size $10$. The gt image was obtained by computing the multicut based on the ground truth edges\relax }}{43}{figure.caption.15}}
\newlabel{fig:resa_dice}{{4.6}{43}{Result using the unsupervised reward after $80000$ optimization steps on mini batches of size $10$. The gt image was obtained by computing the multicut based on the ground truth edges\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data conformity}{43}{subsubsection*.17}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces History of predicted means for the policy (unsupervised reward setting)\relax }}{44}{figure.caption.16}}
\newlabel{fig:resa_dice}{{4.7}{44}{History of predicted means for the policy (unsupervised reward setting)\relax }{figure.caption.16}{}}
\bibstyle{IEEEtran}
\bibdata{bibliography.bib}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{45}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:conclus}{{5}{45}{Conclusion}{chapter.5}{}}
\newlabel{sec:ref}{{5}{45}{Conclusion}{section*.18}{}}
\@writefile{toc}{\contentsline {section}{References}{45}{section*.18}}
\bibcite{wang2016sample}{1}
\bibcite{haarnoja2018soft}{2}
\bibcite{SB_all}{3}
\bibcite{SBQL}{4}
\bibcite{SBeligibility}{5}
\bibcite{liu2018breaking}{6}
\bibcite{PGBS}{7}
\bibcite{AAAIziebert}{8}
\bibcite{DBLP:journals/corr/HaarnojaTAL17}{9}
\bibcite{DBLP:journals/corr/abs-1906-02771}{10}
\bibcite{papamakarios2019normalizing}{11}
\bibcite{hessel2017rainbow}{12}
\bibcite{DBLP:journals/corr/HasseltGS15}{13}
\bibcite{schaul2015prioritized}{14}
\bibcite{DBLP:journals/corr/WangFL15}{15}
\bibcite{mnih2016asynchronous}{16}
\bibcite{bruna2013spectral}{17}
\bibcite{Bronstein_2017}{18}
\bibcite{gilmer2017neural}{19}
\bibcite{wolf2019mutex}{20}
\bibcite{10.1007/978-3-642-23094-3_3}{21}
\bibcite{Sudre_2017}{22}
\bibcite{brab2017semantic}{23}
\bibcite{Schroff_2015}{24}
\bibcite{kingma2013autoencoding}{25}
\bibcite{10.1145/361237.361242}{26}
\bibcite{monti2016geometric}{27}
\bibcite{ronneberger2015unet}{28}
\bibcite{densestSg}{29}
\bibcite{Fey/Lenssen/2019}{30}
\bibcite{ioffe2015batch}{31}
\bibcite{kingma2014adam}{32}
\bibcite{400568}{33}
