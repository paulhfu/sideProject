\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\babel@aux{USenglish}{}
\citation{SB_all}
\citation{wolf2019mutex}
\citation{brab2017semantic}
\citation{Schroff_2015}
\newlabel{sec:ref}{{}{6}{}{section*.1}{}}
\@writefile{toc}{\contentsline {section}{Summary of Notation}{6}{section*.1}}
\@writefile{toc}{\contentsline {section}{\numberline {0.1}Introduction}{7}{section.0.1}}
\newlabel{sec:introduction}{{0.1}{7}{Introduction}{section.0.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Motivation}{8}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:rl_for_seg}{{1}{8}{Motivation}{chapter.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Preliminaries}{9}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{sec:preliminaries}{{2}{9}{Preliminaries}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Image segmentation}{9}{section.2.1}}
\newlabel{sec:prel_imagesegmentation}{{2.1}{9}{Image segmentation}{section.2.1}{}}
\citation{SB_all}
\citation{SB_all}
\citation{SB_all}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Reinforcement Learning (RL)}{10}{section.2.2}}
\newlabel{ssec:rl}{{2.2}{10}{Reinforcement Learning (RL)}{section.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces agent environment interaction \cite  {SB_all}\relax }}{11}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig_rl_gen}{{2.1}{11}{agent environment interaction \cite {SB_all}\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Value functions}{11}{subsection.2.2.1}}
\citation{SBQL}
\citation{SBeligibility}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Q-learning}{12}{subsection.2.2.2}}
\citation{liu2018breaking}
\citation{PGBS}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Policy gradient methods}{13}{subsection.2.2.3}}
\citation{AAAIziebert}
\citation{DBLP:journals/corr/HaarnojaTAL17}
\citation{DBLP:journals/corr/abs-1906-02771}
\citation{papamakarios2019normalizing}
\citation{haarnoja2018soft}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Maximum Entropy Reinforcement Learning}{14}{subsection.2.2.4}}
\citation{haarnoja2018soft}
\citation{kingma2013autoencoding}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.5}Soft Actor-Critic (SAC)}{15}{subsection.2.2.5}}
\newlabel{ssec:sac}{{2.2.5}{15}{Soft Actor-Critic (SAC)}{subsection.2.2.5}{}}
\citation{haarnoja2018soft}
\citation{hessel2017rainbow}
\citation{DBLP:journals/corr/HasseltGS15}
\citation{DBLP:journals/corr/HasseltGS15}
\citation{schaul2015prioritized}
\citation{SBQL}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.6}Common optimization methods}{16}{subsection.2.2.6}}
\newlabel{ssec:common_opt}{{2.2.6}{16}{Common optimization methods}{subsection.2.2.6}{}}
\newlabel{text:doublQ}{{2.2.6}{16}{Common optimization methods}{subsection.2.2.6}{}}
\citation{SBQL}
\citation{DBLP:journals/corr/WangFL15}
\citation{mnih2016asynchronous}
\citation{bruna2013spectral}
\citation{Bronstein_2017}
\citation{gilmer2017neural}
\newlabel{ssec::a3c}{{2.2.6}{17}{Common optimization methods}{equation.2.2.31}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.7}Reparameterization}{17}{subsection.2.2.7}}
\newlabel{ssec:reparam}{{2.2.7}{17}{Reparameterization}{subsection.2.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.8}Normalizing flows}{17}{subsection.2.2.8}}
\newlabel{ssec:norm_flows}{{2.2.8}{17}{Normalizing flows}{subsection.2.2.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.9}The Heugh transorm}{17}{subsection.2.2.9}}
\newlabel{ssec:heugh_tf}{{2.2.9}{17}{The Heugh transorm}{subsection.2.2.9}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Geometric deep learning}{17}{section.2.3}}
\newlabel{sec:gcn}{{2.3}{17}{Geometric deep learning}{section.2.3}{}}
\citation{wolf2019mutex}
\citation{wolf2019mutex}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Mutex watershed}{18}{section.2.4}}
\newlabel{sec:mtx_wtsd}{{2.4}{18}{Mutex watershed}{section.2.4}{}}
\citation{wolf2019mutex}
\citation{wolf2019mutex}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Some iterations of algorithm \ref  {algo:mtx_wtsd} applied to a graph with weighted attractive edges (green) and repulsive (ref) edges. Edges that are part of the active set $A$ at each iteration are shown in bold. On termination (f), the connected components in $A \cap E^+$ represent the partitions of the final partitioning. Edges that are not added to $A$ because of the violation of $\mathcal  {C}_0$ or $\mathcal  {C}_1$ are highlighted in blue and yellow respectively \cite  {wolf2019mutex}\relax }}{19}{figure.caption.3}}
\newlabel{fig_mtxwtsd1}{{2.2}{19}{Some iterations of algorithm \ref {algo:mtx_wtsd} applied to a graph with weighted attractive edges (green) and repulsive (ref) edges. Edges that are part of the active set $A$ at each iteration are shown in bold. On termination (f), the connected components in $A \cap E^+$ represent the partitions of the final partitioning. Edges that are not added to $A$ because of the violation of $\mathcal {C}_0$ or $\mathcal {C}_1$ are highlighted in blue and yellow respectively \cite {wolf2019mutex}\relax }{figure.caption.3}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Mutex Watershed \cite  {wolf2019mutex}\relax }}{19}{algocf.1}}
\newlabel{algo:mtx_wtsd}{{1}{19}{Mutex watershed}{algocf.1}{}}
\citation{wolf2019mutex}
\citation{wolf2019mutex}
\citation{10.1007/978-3-642-23094-3_3}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Image partitioning by multicuts}{20}{section.2.5}}
\newlabel{sec:multicut}{{2.5}{20}{Image partitioning by multicuts}{section.2.5}{}}
\citation{10.1007/978-3-642-23094-3_3}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Principal component analysis}{21}{section.2.6}}
\newlabel{sec:pca}{{2.6}{21}{Principal component analysis}{section.2.6}{}}
\citation{brab2017semantic}
\citation{brab2017semantic}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Loss functions}{22}{section.2.7}}
\newlabel{ssec:losses}{{2.7}{22}{Loss functions}{section.2.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}Dice loss}{22}{subsection.2.7.1}}
\newlabel{ssec:loss_dice}{{2.7.1}{22}{Dice loss}{subsection.2.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}Contrasive loss}{22}{subsection.2.7.2}}
\newlabel{ssec:loss_contrastive}{{2.7.2}{22}{Contrasive loss}{subsection.2.7.2}{}}
\citation{brab2017semantic}
\citation{brab2017semantic}
\citation{Schroff_2015}
\citation{Schroff_2015}
\citation{Schroff_2015}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces As defined by the loss, this are the hinged inter-pulling and intra-pushing forces acting on points in the embedding space \cite  {brab2017semantic}\relax }}{24}{figure.caption.4}}
\newlabel{fig_contrastive}{{2.3}{24}{As defined by the loss, this are the hinged inter-pulling and intra-pushing forces acting on points in the embedding space \cite {brab2017semantic}\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.3}Triplet loss}{24}{subsection.2.7.3}}
\newlabel{ssec:loss_triplet}{{2.7.3}{24}{Triplet loss}{subsection.2.7.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Position of triplets in the embedding space before and after training \cite  {Schroff_2015}\relax }}{24}{figure.caption.5}}
\newlabel{fig_triplet}{{2.4}{24}{Position of triplets in the embedding space before and after training \cite {Schroff_2015}\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Methods}{26}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:rl_for_seg}{{3}{26}{Methods}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Using RL for the image segmentation task}{26}{section.3.1}}
\newlabel{sec:rl_for_seg}{{3.1}{26}{Using RL for the image segmentation task}{section.3.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Using RL for predicting affinities}{26}{section.3.2}}
\newlabel{sec:rl_for_seg}{{3.2}{26}{Using RL for predicting affinities}{section.3.2}{}}
\citation{10.1007/978-3-642-23094-3_3}
\citation{monti2016geometric}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Overview over the pipeline}{27}{section.3.3}}
\newlabel{seg:ov_pip}{{3.3}{27}{Overview over the pipeline}{section.3.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Rough sketch of the proposed pipeline. Starting from raw data (top left), a superpixel graph is obtained with mutex watershed \ref  {algo:mtx_wtsd} and pixel embeddings (top right) with an embedding network. Computing node features based on the average pixel embedding per superpixel a GCNN predicts logits on the edges of the superpixel graph. The logits are used to compute chances which in turn are used to compute costs based on which a multicut of the superpixel graph is computed and a segmentation is obtained from the multicut and the region adjacency graph of the superpixels. This segmentation is then evaluated and a reward is produced which is then used in the RL loss.\relax }}{28}{figure.caption.6}}
\newlabel{overview}{{3.1}{28}{Rough sketch of the proposed pipeline. Starting from raw data (top left), a superpixel graph is obtained with mutex watershed \ref {algo:mtx_wtsd} and pixel embeddings (top right) with an embedding network. Computing node features based on the average pixel embedding per superpixel a GCNN predicts logits on the edges of the superpixel graph. The logits are used to compute chances which in turn are used to compute costs based on which a multicut of the superpixel graph is computed and a segmentation is obtained from the multicut and the region adjacency graph of the superpixels. This segmentation is then evaluated and a reward is produced which is then used in the RL loss.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}The pipeline in the RL terminology}{29}{section.3.4}}
\newlabel{seg:pip_tl_term}{{3.4}{29}{The pipeline in the RL terminology}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}The state}{29}{subsection.3.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}The actions}{29}{subsection.3.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}The reward}{29}{subsection.3.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}The agent}{30}{subsection.3.4.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.5}The environment}{30}{subsection.3.4.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.6}The problem of local optima}{30}{subsection.3.4.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces A rough sketch of the reward calcuation on subgraphs with 7 edges and the resulting losses in an actor critic setting\relax }}{31}{figure.caption.7}}
\newlabel{overview}{{3.2}{31}{A rough sketch of the reward calcuation on subgraphs with 7 edges and the resulting losses in an actor critic setting\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.7}Definition of the RL algorithm}{31}{subsection.3.4.7}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Obtaining superpixels from mutex watershed}{32}{section.3.5}}
\newlabel{seg:pip_mutex}{{3.5}{32}{Obtaining superpixels from mutex watershed}{section.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}The embedding network}{32}{section.3.6}}
\newlabel{seg:pip_embed}{{3.6}{32}{The embedding network}{section.3.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}The actor critic networks}{33}{section.3.7}}
\newlabel{sec:sag_gcn}{{3.7}{33}{The actor critic networks}{section.3.7}{}}
\citation{densestSg}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Finding subgraphs}{36}{section.3.8}}
\newlabel{seg:sag_gcn}{{3.8}{36}{Finding subgraphs}{section.3.8}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Dense subgraphs in a rag\relax }}{37}{algocf.2}}
\newlabel{algo:sgs}{{2}{37}{Finding subgraphs}{algocf.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8.1}Thoughts on dependence}{37}{subsection.3.8.1}}
\citation{Fey/Lenssen/2019}
\citation{ioffe2015batch}
\@writefile{toc}{\contentsline {section}{\numberline {3.9}Technical details}{38}{section.3.9}}
\newlabel{sec:tech}{{3.9}{38}{Technical details}{section.3.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.9.1}Batch processing}{38}{subsection.3.9.1}}
\newlabel{ssec:batchp}{{3.9.1}{38}{Batch processing}{subsection.3.9.1}{}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Experiments and results}{39}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:exp_res}{{4}{39}{Experiments and results}{chapter.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Example raw image\relax }}{39}{figure.caption.8}}
\newlabel{fig:exmpl}{{4.1}{39}{Example raw image\relax }{figure.caption.8}{}}
\citation{kingma2014adam}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Values in first three principal components of pixel embeddings\relax }}{40}{figure.caption.9}}
\newlabel{fig:pixembeddings}{{4.2}{40}{Values in first three principal components of pixel embeddings\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.1}Supervised training}{40}{subsection.4.0.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Result using the dice reward after $80000$ optimization steps on mini batches of size $10$. The gt image was obtained by computing the multicut based on the ground truth edges\relax }}{41}{figure.caption.10}}
\newlabel{fig:resa_dice}{{4.3}{41}{Result using the dice reward after $80000$ optimization steps on mini batches of size $10$. The gt image was obtained by computing the multicut based on the ground truth edges\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces Visualization of the values of the edge features is their first $3$ principal components (PCA1-3) and the ground truth edge values (GT) after training with the dice rewards for $80000$ optimization steps on mini batches of size $10$.\relax }}{41}{figure.caption.11}}
\newlabel{fig:res_edge_embed}{{4.4}{41}{Visualization of the values of the edge features is their first $3$ principal components (PCA1-3) and the ground truth edge values (GT) after training with the dice rewards for $80000$ optimization steps on mini batches of size $10$.\relax }{figure.caption.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces History of predicted means for the policy (supervised reward setting)\relax }}{41}{figure.caption.12}}
\newlabel{fig:pred_means}{{4.5}{41}{History of predicted means for the policy (supervised reward setting)\relax }{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.0.2}Unsupervised training}{42}{subsection.4.0.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Result using the unsupervised reward after $80000$ optimization steps on mini batches of size $10$. The gt image was obtained by computing the multicut based on the ground truth edges\relax }}{42}{figure.caption.13}}
\newlabel{fig:resa_dice}{{4.6}{42}{Result using the unsupervised reward after $80000$ optimization steps on mini batches of size $10$. The gt image was obtained by computing the multicut based on the ground truth edges\relax }{figure.caption.13}{}}
\@writefile{toc}{\contentsline {subsubsection}{Data conformity}{42}{subsubsection*.15}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces History of predicted means for the policy (unsupervised reward setting)\relax }}{43}{figure.caption.14}}
\newlabel{fig:resa_dice}{{4.7}{43}{History of predicted means for the policy (unsupervised reward setting)\relax }{figure.caption.14}{}}
\bibstyle{IEEEtran}
\bibdata{bibliography.bib}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Conclusion}{44}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{loa}{\addvspace {10\p@ }}
\newlabel{chap:conclus}{{5}{44}{Conclusion}{chapter.5}{}}
\newlabel{sec:ref}{{5}{44}{Conclusion}{section*.16}{}}
\@writefile{toc}{\contentsline {section}{References}{44}{section*.16}}
\bibcite{SB_all}{1}
\bibcite{wolf2019mutex}{2}
\bibcite{brab2017semantic}{3}
\bibcite{Schroff_2015}{4}
\bibcite{SBQL}{5}
\bibcite{SBeligibility}{6}
\bibcite{liu2018breaking}{7}
\bibcite{PGBS}{8}
\bibcite{AAAIziebert}{9}
\bibcite{DBLP:journals/corr/HaarnojaTAL17}{10}
\bibcite{DBLP:journals/corr/abs-1906-02771}{11}
\bibcite{papamakarios2019normalizing}{12}
\bibcite{haarnoja2018soft}{13}
\bibcite{kingma2013autoencoding}{14}
\bibcite{hessel2017rainbow}{15}
\bibcite{DBLP:journals/corr/HasseltGS15}{16}
\bibcite{schaul2015prioritized}{17}
\bibcite{DBLP:journals/corr/WangFL15}{18}
\bibcite{mnih2016asynchronous}{19}
\bibcite{bruna2013spectral}{20}
\bibcite{Bronstein_2017}{21}
\bibcite{gilmer2017neural}{22}
\bibcite{10.1007/978-3-642-23094-3_3}{23}
\bibcite{monti2016geometric}{24}
\bibcite{densestSg}{25}
\bibcite{Fey/Lenssen/2019}{26}
\bibcite{ioffe2015batch}{27}
\bibcite{kingma2014adam}{28}
