\section{Using RL for the image segmentation task}~\label{sec:rl_for_seg}

In this section the task of image segmentation is fit to the RL framework.\\
The agent takes the role of predicting action distributions where sampled actions perform some kind of change to the input of a segmentation algorithm. Rewards can be calculated from the resulting segmentation. The segmentation algorithm would therefore be part of the environment. \\
In order to optimize the agents parameters by the RL losses, the segmentation algorithm can be non differentiable.\\
Common RL problems are usually problems that are easy to evaluate like determining the winner of a board game or evaluating a robots position relative to a target position. Also often intermediate results are not rewarded at all or given a small negative reward in order to put pressure on the fast arrival of the final state.\\
Typically in RL there is a physical environment that can be measured by sensory input a reward calculation based on that signals can take place. For the task of image segmentation such a environment can not be found or measured. The labeling would have to be projected from the image plane into the "real world" where measurements could take place. Of course it is not clear how to do that, therefore the reward generation has to be based on a virtual model of the environment. A segmentation can only be evaluated if enough information on the objects within the input image is known a priori. E.g number of object instances per object class, position, texture, shape etc..\\
Given such an evaluation and given that the function that is optimized by the agent is capable of capturing the underlying probability distribution of data label pairs, the described RL setting is theoretically able to learn the task of image segmentation in a fully unsupervised way.\\

