\section{The pipeline in the RL terminology}~\label{seg:pip_tl_term}

As shown in \ref{fig_rl_gen} in all RL problems there are two main instances acting and sending signals to each other. A closer look at each instance and signal as well as their definition within the context of the task is given below. 

\subsection{The state}
Letting the current state be the concatenation of the raw data, a superpixel segmentation $sp$ of the raw data $rw$ and a final segmentation $seg_t$ that is a partitioning of the superpixels $s_t=\left[ rw, sp, seg_t \right]$. Therefore the only part of the state that is ever updated during training on a single image $rw$ is $seg_t$. That means, the only part of the state space that needs to be explored by the agent is defined by $seg_t$. If the segmentation would not be based on superpixels the number of possible states would be a much higher in constrast to $seg_t$ relating to a superpixel partitioning. The state space is the search space in which only one point correponds to the ground-truth.

\subsection{The actions}
The formulation of the actions depend on the RL algorithm used. There are algorithms for discrete and for continuous action spaces. The predicted target are probabilities for merge affinities, therefore values between 0 and 1. So it would be natural to predict continuous actions between 0 and 1 that can then directly be taken for the target merge affinity.\\
However most algorithms with a policy based on value functions like Q-learning are defined only for discrete actions. Algorithms incorporating policy gradients can usually defined for both discrete and continuous action spaces. When working with discrete actions there are 2 possibilities of their definition. \\
\begin{itemize}
	\item one possibility is to directly predict values for discretisized probabilities. Depending on the degree of discretization this might lead to large complexity. Also this makes it possible to diverge away from an initial state very fast, namely within one step. If there is an initial state which is likely already close to the ground truth this is not favorable.
	\item the other possibility is to predict values for actions that are operating on the current state of edge values. E.g a state action value to add or subtract a fixed scalar $c$. Here the level of discretization depends on the magnitude of $c$ which does not change the memory complexity of the output however it has a direct affect on the number of steps that are neccessary to arrive at a target state. This method also favors a more continuous divergence from an initial state.
\end{itemize}

\subsection{The reward}
The reward is crucial for the whole training behavior. The right modelling of the reward signal principally decides for fast convergence to the target solution and the avoidance of "getting stuck" in local optima.\\
If a set of training image-label pairs is available it makes sense to derive a ground truth value for every edge in the superpixel graph. Then the reward is per edge as the distance of the current edge state to the ground truth edge. This is the most accurate reward that can be obtained. Nevertheless this version comes with the drawback of generating large variance in the updates of the state action value function. The contrast to this would be the prediction of a single state action value. This certainly smoothes out any variance in the single predictions but it is also too coarse when it comes to larger superpixel graphs. This problem is in more detail attendet to in section \ref{blabla}.

\subsection{The agent}
The role of the agent is taken mainly by the embedding network, the GCNN and the optimization of those. Its input is the input to the embedding network which is the current state $s_t$. It outputs statistics of a probability distribution per edge. Depending of the choice of algorithm this can be arrays of probabilities for a categorical probability distribution in the case of discrete actions, or the statistics of a proibability desity fucntion in the case of continuous actions. The latter requires a sigmoid transformation of the samples to guarantee they fit the requirement of being a probability.

\subsection{The environment}
The environment receiving actions $a_t$ that act on a state $s_t$ producing $s_{t+1}$ as well as a reward $r_t$. Therefore it mainly consists of the multicut algorithm updating the state based on the actions and of some evaluation scheme for the new state in order to calculate rewards. This scheme can be based on ground truth segmentations or on prior knowledge or both.

\subsection{The problem of local optima}
Usually the ground truth of edge weights reveils a inbalance in attractive and repulsive edges. Due to the nature of an oversegmentation there are a more attractive edges than repulsive edges. This inbalance generates the local optimum of exclusively attractive edges. RL algorithms are known for converging to local optima and also perturbations of the rewards are not able to prevent this.\\
This kind of local optimum is known in image segmentation problems and has been adressed by many losses like focal loss aor dice loss. The dice score can easily directly be transferred to edge value predictions. The problem here is that this produces a single scalar reward. This is a problem because there can easily be a few hundreds or even thousands of edges within a superpixel graph. Having a scalar reward signal is to vague to infer from to actions on single edge values.\\
Most RL benchmarks incorporate action dimensions that are less than $10$ which is a small enough number to have one global state action value.\\
Transferring this to the prediction of edge values on a graph would be a single state action value per subgraph of roughly size $10$. This has the advantage of training a state action value function globally for the predictions on each subgraph. Therefore, if ground truth is available, one can use the dice score over a subgraph as a reward signal. This smoothes out class inbalances as well as variances of single edge state action values. This method is shown in figure \ref{fig_sg}. The figure also roughly sketches method to compute per subgraph rewards in an unsupervised fashion. The subraphs can and should overlap in order to smooth out variances in the reward signal. Since a GCNN is used for the state action value prediction it makes sense to select subgraphs with a high density which increases the information flow in the graph convolution.

\begin{figure}[ht!]
	\centering
	\includegraphics[width=1.5\textwidth]{figures/images/reward_calc_sketch.png}
	\caption{A rough sketch of the reward calcuation on subgraphs with 7 edges and the resulting losses in an actor critic setting}
	\label{overview}
\end{figure}

\subsection{Definition of the RL algorithm}
Vanilla Q-learning or REINFORCE are algorithms that operate in discrete action spaces. The advantages of that have been mentioned. However for the prediction of probabilities it is more natural to use a continuous action space. The drawback is that it is possible to diverge fast from an initial state. Such a divergence can easily be penalized by the reward signal e.g by calculating the distance of current state to the initial state and subtracting that distance from the reward when it surpasses a certain margin. \\
Therefore the selection falls to the SAC algorithm \ref{ssec:sac} which is a confortable choice because it is defined for continuous actions and it takes care of sufficient exploration. 

It is easy to adjust eq. (2.26) in section \ref{ssec:sac} for predictions on subgraphs. Considering a subgraph size of $10$ and selecting the normal distribution for the policy $\pi$. The GCNN predicts for every edge mean $\mu$ and variance $\sigma ^2$ of its action distribution. \\
Drawing a reparameterized \ref{ssec:reparam} sample from the distribution follows a sigmoid transform of the sample. The change of variables formula \ref{ssec:norm_flows} allows for the computation of the probability density of the transformed sample. \\
The joint probability density of all actions per subgraph is given by the product of their respective densities. Therefore eq. (2.26) in section \ref{ssec:sac} is rewritten as

\begin{align}
	\nabla_\theta \bar{\mathcal{L}}_{actor} = \nabla_\theta \sum_{sg \in G} \left[ \alpha \sum_{a_t\in sg}log(\pi(a_t|s_t)) - Q_\pi(s_t, a_t)_{sg} \right]
\end{align}

Here $G$ is the set of sets that contain the respective actions for each subgraph. $Q_\pi(s_t, a_t)$ is a function taking the current state action tuple and maps it to $\mathbb{R}^n$ where $n$ is the number of subgraphs in $s_t$. $Q_\pi(s_t, a_t)_{sg}$ denotes the predicted state action value for subgraph $sg$.\\
eq. (2.22) in section \ref{ssec:sac} does not change considering the rewards are per subgraph as well.\\
Additionally to the optimization techniques within the SAC \ref{ssec:sac} algorithm, prioritized experience replay \ref{ssec:common_opt} is used.\\
RL problems are usually of the form that multiple steps lead to an end state $T$. Since here, directly sampling affinity values from the policy makes it possible to reach any state within one step, it is sufficient to define $T=1$. Stopping after one step has the advantage that the state action function becomes much simpler. The feature to be able to reach any state from any other state makes all parts of the state that are dependent on $t$ redundant. Therefore $s_t$ can be redefined to $s_t=s=[rw, sp]$.\\
Setting $T=1$ the loss in e(2.22) becomes

\begin{align}
	\mathcal{L}_{critic} = \frac{1}{2}(Q_{\pi}(s_t, a_t) - r_t) ^ 2
\end{align}

While this yields a simple action state function to approximate, there is also a point in saying that this definition is not a "real" RL setting anymore. However the RL loss still gives the advantage that the supervision signal (the reward here), does not have to be differentiable, which is the main justification for this pipeline.