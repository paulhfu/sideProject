\section{Technical details}\label{sec:tech}
This section reviews some technical details of the implementation of the pipeline. Except for algorithm \ref{algo:sgs} and some more helper functions for operations on graphs that have been implemented in c++, everything has been implemented in python under heavy use of the libraries pytorch, numpy, scipy and skimage. The c++ implementations are called by the python interpreter using the pybind11 interface together with the xtensor libraries. The GCNNs have been implemented using the library pytorch-geometric, introduced in \cite{Fey/Lenssen/2019}.\\
pytorch multiprocessing is used for parallelization and synchronization in the fashion of the A3C (see section \ref{ssec::a3c}). After each update step through graph convolutions in section \ref{sec:sag_gcn}, node and edge features are normalized by a Batch Normalization layer \cite{ioffe2015batch}.\\
The embedding space is $\mathbb{R}^{16}$, therefore the node features in section \ref{sec:sag_gcn} are $x_{i}^0 \in \mathbb{R}^{16}$. The number of convolution iterations in section \ref{sec:sag_gcn} is $K=5$ as well as for $M=5$. The number of convolution iterations for the subgraph GCNNs is $N=10$. There are more convolution iterations on the subgraphs, because of the downstream global pooling operation, it is important that the information in the graph is spread over all nodes.\\
The multi layer perceptrons in eqs. (3.3-3.14) in section \ref{sec:sag_gcn} have all $1$ hidden layer. The first multi layer perceptron in each of the actors convolution blocks has $in=16\cdot 2$ input features and $in\cdot 10$ output features. All the following except the last one have $in\cdot 10$ input and $in\cdot 10$ output features. The last one squashes the feature vectors again to $16$ dimensions for node and edge features. For the critic's first networks that operate on the whole graph, the same structure holds, except here the input features are $in=16\cdot 2 + 1$ since the actions $a_{ij}$ are concatenated to the node features $x_{i}^0$.


\subsection{Batch processing}\label{ssec:batchp}
Batching a set of irregular graphs can be achieved by converting the set of graphs into one large graph whose connected components correspond to each graph in the batched set. Doing graph convolution on the large graph yields the same result as doing the convolution on each of the batched graphs separately with the difference that the convolutions can be performed in parallel. This batching method is used for the convolution on the set of subgraphs.