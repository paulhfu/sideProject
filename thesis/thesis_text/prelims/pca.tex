\section{Principal component analysis}~\label{sec:pca}
The principal components of a collection of data points can be thought of as the directions in which the variance of the data points is the highest. The magnitude of the variance in a specific direction is referred to as the scores of the respective principal component. All principal component vectors form a orthonormal basis.\\
Consider a data matrix $X \in \mathbb{R}^{nxp}$ of $n$, $p$-dimensional samples from a arbitrary distribution. The first principal components is the arg-maximizer

\begin{align}
	w_{(1)} = \argmax_{\norm{w} = 1} \norm{Xw}^2
\end{align}

Since this is a convex optimization problem, the solution can be found by finding the stationary points of the Lagrange function

\begin{align}
	\mathcal{L}(w, \lambda) = w^TCw - \lambda(w^Tw -1) \\
	\text{with \hspace{2mm}} C = X^TX
\end{align}
Note, that $C$ is hermetian.
The partial derivatives yield	
\begin{align}
	\nabla_w \mathcal{L}(w, \lambda) = 2Cw-2\lambda w \\
	\nabla_\lambda \mathcal{L}(w, \lambda) = - (w^Tw -1)
\end{align}

setting eq. (1.41) to $0$ yields

\begin{align}
	0 = Cw-\lambda w \\
	Cw = \lambda w
\end{align}

it strikes that eq. (1.43) is a eigenvalue problem. Since $C$ is hermetian its eigenvectors form a orthonormal basis, therefore setting eq. (1.42) to $0$ holds if $w$ is a eigenvector. To see that the solution to eq. (1.38) is in fact the largest eigenvalue, one has to substitute eq. (1.44) into eq. (1.40).

\begin{align}
	w^TCw - \lambda(w^Tw -1) = w^TCw = \lambda w^Tw = \lambda
\end{align}

Since eq. (1.40) is a maximization problem, $\lambda$ has to be the largest eigenvalue.
Therefore the first principal component is the eigenvector $w_{(1)}$ with the largest eigenvalue $\lambda_{(1)}$ of $C$. $\lambda_{(1)}$ is also referred to as the score of the principal component $w_{(1)}$. \\

The remaining principal components are given by the remaining eigenvectors sorted by their eigenvalues.
