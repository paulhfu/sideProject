\section{Loss functions}~\label{ssec:losses}
This section reviews some important loss functions.

\subsection{Generalized dice loss}\label{ssec:loss_dice}
The dice score is a measure based on relative spatial overlap of a prediction and a binary ground truth. The generalized dice loss as defined in \cite{Sudre_2017} defines the dice score for the evaluation of a prediction for multiple classes.\\
For images with $N$ pixels and $L$ classes, let $y_{ln}$ be the binary ground truth label for a pixel $n \in N$ and a class $l \in L$ and let $p_{ln}$ be the prediction for a pixel $n \in N$ and a class $l \in L$. Then the dice loss is defined as

\begin{align}
	\mathcal{L} = 1 - 2 \frac{\sum_{l=1}^L w_l \sum_{n=1}^N y_{ln} p_{ln}}{\sum_{l=1}^L w_l \sum_{n=1}^N y_{ln} + p_{ln}}
\end{align}

where $w_l$ are weights for the corresponding classes.

\subsection{Contrasive loss}\label{ssec:loss_contrastive}

This loss, as defined in \cite{brab2017semantic}, applies to the task of instance segmentation in images. A differentiable function predicts points in a feature space that is embedded in a $n$-dimensional eucledian space. Each predicted point in the embedding space corresponds to a pixel in the image. Ideally the $n$-dimensional embedding vectors for each pixel are close to each other in the embedding space if the corresponding pixels belong to the same instance and distant to each other if not. If that is the case, a clustering algorithm can assign an instance/cluster to each embedding vector.\\
The loss penalizing predictions can be thought of as a force that pulls pixel embeddings of the same instance together and pushes those of different instances apart.
This loss consists of three additive parts. Assuming $C$ is the number of instances in an image (each label id in the ground truth image represents an instance except for the background label id). $N_C$ is the number of elements in cluster $c \in [1..C]$, $x_i$ is an embedding vector where $i$ denotes a pixel. $\mu_c$ is the mean embedding of cluster $c$, $\norm{\cdot}$ is the $L1$ or $L2$ distance and $[x]_+ = \max(0, x)$. $\delta_v$ and $\delta_d$ are margins that are used to hinge the pull and push forces. That is, the forces are only applied if the distance that is under consideration is larger than $\delta_v$ for intra cluster pulling forces, and smaller than $2\delta_d$ for inter cluster pushing forces. This allows to learn a more expressive embedding space since the points are allowed to move freely if there are no forces applied on them. The three additive terms of the final loss are\\

\begin{itemize}
	\item \textbf{variance term}\\
	this exerts a intra cluster pull force that draws pixel embeddings towards the cluster center of their respective instance if the distance to the center is larger than $\delta_v$.
	
	\begin{align}
		\mathcal{L}_{var} = \frac{1}{C} \sum_{c=1}^C \frac{1}{N_c} \sum_{i=1}^{N_c} \left[ \norm{\mu_c - x_i} - \delta_v \right]_+^2
	\end{align}
	
	\item \textbf{distance term}\\
	this exerts a inter cluster push force that pushes clusters away from each other by penalizing distances between cluster centers that are smaller than $2\delta_d$.
	
	\begin{align}
		\mathcal{L}_{dist} = \frac{1}{C(C-1)} \sum_{c_A=1}^C \sum_{\substack{c_B=1 \\ c_B \neq c_A}}^C \left[ 2\delta_d - \norm{\mu_{c_A} - \mu_{c_B}} \right]_+^2
	\end{align}
	
	\item \textbf{regularization term}\\
	this is a small pull force that keeps the predicted embedding vectors close to the origin.
	
	\begin{align}
		\mathcal{L}_{reg} = \frac{1}{C} \sum_{c=1}^C \norm{\mu_c}
 	\end{align}
\end{itemize}

Then the final loss yields

\begin{align}
	\mathcal{L} = \alpha \mathcal{L}_{var} + \beta \mathcal{L}_{dist} + \gamma \mathcal{L}_{reg}
\end{align}

where $\alpha$, $\beta$ and $\gamma$ are weights for the respective terms.\\

The behavior for the different terms in the loss is sketched in figure \ref{fig_contrastive}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.5\textwidth]{figures/contrastive_loss.png}
	\caption{\cite{brab2017semantic} As defined by the loss, this are the hinged inter pulling and intra pushing forces acting on points in the embedding space}
	\label{fig_contrastive}
\end{figure}

\subsection{Triplet loss}\label{ssec:loss_triplet}
This loss, as defined in \cite{Schroff_2015} is related to the contrastive loss (see section \ref{ssec:loss_contrastive}) in the sense that it is also defined over data points in an embedding space where the resulting embeddings form ideally per instance clusters. The loss function expects three embedding vectors as an input. An anchor $x_i^a$, a embedding that is of the same class $x_i^p$ as the anchor and one that is of a different class $x_i^n$. Then the loss yields

\begin{align}
	\mathcal{L}_{trpl} = \sum_i^N \left[ \norm{x_i^a - x_i^p}^2 - \norm{x_i^a - x_i^n}^2 + \alpha \right]_+ \text{\hspace{1mm}.}
\end{align} 

Again $[x]_+ = \max(0, x)$. $N$ is the number of all possible triplets $(x_i^a, x_i^p, x_i^n) \in \mathcal{T}$ in the embeddings, $\norm{\cdot}$ is the $L2$ norm and $\alpha$ is a enforced margin between positive and negative pairs. The behavior during training using $\mathcal{L}_{trpl}$ as a loss is depicted in figure \ref{fig_triplet}. \\

\begin{figure}[ht!]
	\centering
	\includegraphics[width=0.5\textwidth]{figures/triplet_loss.png}
	\caption{\cite{Schroff_2015} Position of triplets in the embedding space before and after a optimization step}
	\label{fig_triplet}
\end{figure}

Often times $N$ is very large and the loss calculation requires too many resources. Therefore it is crucial to select triples that are active, namely $\left\{ (x_i^a, x_i^p, x_i^n) \bigg| \left[ \norm{x_i^a - x_i^p}^2 - \norm{x_i^a - x_i^n}^2 + \alpha \right]_+ \neq 0 \right\}$.\\  However different selection strategies end in very different results regarding training time and convergence to satisfactory optima. E.g. picking only triplets that produce a high loss value can result in a training converging quickly into bad local optimum.\\
To prevent the embeddings from diverging, a regularizing term is added in order to constrain the embedding vectors to the unit hypersphere surface.
\begin{align}
	\mathcal{L}_{reg} = \sum_i^M\frac{1}{2}(\norm{x_i} - 1)^2
\end{align}

Here $M$ is the number of pixels in the image. The final loss therefore yields

\begin{align}
\mathcal{L} = \mathcal{L}_{trpl} + \beta \mathcal{L}_{reg}
\end{align}

where $\beta$ is a scalar weight for the regularizer.