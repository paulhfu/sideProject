\section{Image segmentation}~\label{sec:prel_imagesegmentation}

Image segmentation or image partitioning is defined as the process of dividing a digital image into sets of pixels also known as the objects in an image.\\
There are many variations of the segmentation method itself as well as of the goal that is to be achieved. With the rise of neural networks, nowadays these methods are usually fully or py parts learning based. That means, that some parameterized function is used to approximate a target distribution which can only be described by some sparse prior knowledge and/or by drawing samples from it. For the task of image segmentation these samples are of the form $(x, y)$ which is a realization of a random variable $(X, Y)$ with joint probability distribution $P_{X, Y}$. Samples respresent the raw input image that is to be segmented $x$ and the desired segmentation also referred to as label or label image $y$. The goal is to learn a function $f(x)$ such that it approximates $\mathbb{E}[Y|X=x]$ at best. Since $P_{X, Y}$ is initially not known and only few samples and/or some sparse prior knowledge on its properties are available, the approximation can only be achieved by the empirical expectation from those samples and by dexterous use of the prior knowledge. \\
Some of the variations of learning based methods for image segmentation are distinguished by their level of supervision during the optimization. This is mainly defined by the amount of data samples and prior knowledge on the distribution $P_{X, Y}$ that is available.

\begin{itemize}
	\item \textbf{supervised segmentation} is the highest level of supervision. Here only samples from $P_{X, Y}$ are available to the method. If enough samples are available such that all regions in the domain of the distributions are covered sufficiently, this is usually all one needs to arrive a a satisfactory result. However for most applications the set of available samples is only very limited.
	\item \textbf{unsupervised segmentation} is the lowest level of supervision. Here no samples from $P_{X, Y}$ are available to the method. The optimization method has to completely rely on prior knowledge on the underlying distribution. Realizations of $X$ are usually still available and can be used for the approximation.
	\item \textbf{semi-supervised segmentation} is the transition between the previous two. Similar to supervised learning, semi-supervised learning uses samples from $P_{X, Y}$ but not only. There are also realizations of $X$ available as well as some prior knowledge on $P_{X, Y}$ which is used during the optimization process as well.
	\item \textbf{self-supervised learning} is usually referred to when the method generates some kind of supervisory signal for itself. E.g. an automated labeling procedure to generate sample approximations $(x, \bar{y})$.
\end{itemize}

variations that focus more on the goal that should be achieved are

\begin{itemize}
	\item \textbf{semantic segmentation} is the process of assigning class labels to each pixel in the image. Different objects of the same class are labeled equally. Usually only some objects of interest get a uique class label assigned to. All other objects receive the label background.
	\item \textbf{instance segmentation} is similar to semantic segmentation in the sense that each pixel in the image is assigned a label to. This label defines it either to background or to an instance of a object class. Therefore different objects of the same class are labelled differently. Here the label of an instance is also referred to as object id.
	\item \textbf{panoptic segmentation} is a fusion of the previous two. For all pixels belonging to instances of some defined set of classes, instance segmentation is performed. For the remaining pixels, semantic segmentation without is performed.
\end{itemize}