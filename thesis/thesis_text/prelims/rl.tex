\section{Reinforcement Learning}~\label{ssec:rl}
Please note that this is an aggressively shortened summary. For a deeper introduction please refer to \cite{SB_all}.
The Reinforcement Learning problem originates from the idea of learning by interacting with an environment. The object that is learning is doing so by retrieving information from cause and effect. The causal model that is learned in such a way is updated with each change of the environment that can be related to some action. Therefore, the learned model fits the true model increasingly better with the number of induced causes and observed effects.\\
This type of learning problem can be modeled by "Finite Markov Decision Processes". Such processes usually need the following elements:\\

\begin{itemize}
	\item \textbf{Environment} The environment is a dynamic model of some complex process.
	\item \textbf{State} The state $S_t$ is generated by the environment. It changes over time according to the dynamics within the environment.
	\item \textbf{Action} An action $A_t$ is a cause that might change the state of the environment. Actions are produced by the agent.
	\item \textbf{Reward} The reward $R_t$ is a scalar value that is produced by the environment and received by the agent.
	\item \textbf{Agent} The agent is a instance which generates actions and observes the caused change of the state of the environment.
	\item \textbf{Policy} A policy $\pi(A_t|S_t)$ is a probability distribution over the set of possible actions at a time step. A agent is essentially defined by its policy as each action that is taken is sampled from that policy.
\end{itemize}

\noindent All signals in this model are time dependent. 
Such a model satisfies the Markov Property if next $s'$ and reward $r$ only depend on the current action-state tuple $(s, a)$. If assuming finite state and action spaces together with the Markov Property gives a Finite Markov Decision Process. The environment dynamics can therefore be represented by the bivariate probability distribution
\begin{align}
	p(s', r|s, a) = Pr\{R_{t+1}=r, S_{t+1} = s' | S_t=s, A_t=a\}
\end{align}
Further, if $A_t$ is sampled from $\pi(A_t|S_t)$ the Markov Property induces conditional independence of $(S_{t-1}, R_{t-1})$ and $(S_{t+1}, R_{t+1})$ given $S_t$. \\

The agents task is to predict a policy that maximizes the expected future rewards. This objective is given by
\begin{align}
	\argmax_{\pi}\mathop{\mathbb{E}}_{p_{\pi}}\left[\sum_{t=0}^T R_t | S_0\right]
\end{align}
Here T marks the time limit of the process and $p_{\pi}$ represents the environment dynamics following a action history sampled from $\pi$.\\

\subsection{Value functions}
Most methods of solving eq. (1.2) use so estimations of so called value functions. This are functions that provide a quality measure for an agent evaluating a state-action tuple. \\
Commonly two value functions are used. The state-value function and the action-value function. They both depend on all future rewards
\begin{align}
	G_t = \sum^{T-t-1}_{k=0} \gamma^kR_{t+k+1} .
\end{align}
Here $\gamma$ is referred to as the discount factor. Its value usually determines how prospective future rewards are weighted in the value function. E.g if $\gamma<1$ rewards that are closer to $t$ get a higher weight than those that are occuring at a later time. For $\gamma>1$ the contrary holds.\\
The state value function is defined by
\begin{align}
	v_{\pi}(s) = \mathop{\mathbb{E}}_{p_{\pi}}\left[G_t|S_t=s \right]
\end{align}
and the action value by
\begin{align}
	q_{\pi}(s, a) = \mathop{\mathbb{E}}_{p_{\pi}}\left[G_t|S_t=s, A_t=a \right]
\end{align}
it follows
\begin{align}
	v_{\pi}(s) = \mathop{\mathbb{E}}_{a\sim\pi}\left[ q_{\pi}(s, a)\right]
\end{align}

\noindent Usually the objective is to maximize either one or both of the value functions.
Note that, $\max_{\pi}v_{\pi}(s)$ and $\max_{\pi}q_{\pi}(s, a)$ satisfy Bellman's principle of optimality. Hence they can be solved exactly by Dynamic Programming. This referred to as the tabular solution. However for most problems this is not feasible and the value functions are approximated by neural networks. \\

\subsection{Q-learning}
Q-learning is a method to find the action-value maximizing policy by using temporal differences. This often the backbone of policy gradient algorithms. Let
\begin{align}
\pi(\cdot|s) = \softmax_{a}q_{\pi}(s, a)
\end{align}
then the objective is to approximate $q_{\pi}$ which is achieved by the temporal difference loss
\begin{align}
\mathcal{L}_{TD} = \left[R_{t+1} + \gamma \max_{a}q_{\pi}(S_{t+1}, a) - q_{\pi}(S_t, a_t)\right]^2
\end{align}
this kind of approximation is usually referred to as one step TD method. The optimality follows directly and the convergence of the approximation under certain conditions has been proven in \cite{SBQL}.\\
In contrast to one step TD methods there are Monte Carlo methods which collect the loss over whole episodes where one episode is defined by the history of temporal differences between a starting state and an end state. This methods are often solved by eligibility traces \cite{SBeligibility}. \\
Optimizing eq (1.8) is referred to as on-policy policy optimization where the target policy $\pi$ is the policy which is used when actions are sampled. This however is problematic as the target policy is defined by eq (1.7) depending on $q_{\pi}$ which is not trustworthy as this is the function that is to be approximated. In order to have more control over the sampling of actions which is usually referred to as exploration a data collection policy $\mu(a|s)$ is used. Therefore in an off-policy setting eq (1.8) becomes
\begin{align}
\mathcal{L}_{TD} = \left[R_{t+1} + \gamma \max_{a}q_{\mu}(S_{t+1}, a) - q_{\mu}(S_t, a_t)\right]^2\text{.}
\end{align}
and during inference 
\begin{align}
\bar{\pi}(\cdot|s) = \softmax_{a}q_{\mu}(s, a)
\end{align}
is used. There are many solutions to overcome the distribution mismatch between $\pi$ and $\mu$. Many use importance sampling or variance reduction techniques.\cite{liu2018breaking}

\subsection{Policy gradient methods}

This class of methods optimizes a policy $\pi_\theta(a|s)$ directly with respect to its parameters $\theta$. Let 
\begin{align}
\rho(\pi) = \mathop{\mathbb{E}}\left[ \sum_{t=1}^{\infty} \gamma^{t-1}R_t|S_0, \pi\right]
\end{align}
be the expected, discounted future reward per step and let
\begin{align}
d_\pi(s) = \sum_{t=0}^\infty \gamma^t Pr\left\{S_t=s|S_0, \pi\right\}
\end{align}
be the discounted stationary distribution of states under $\pi$. Then
\begin{align}
\frac{\partial\rho_\pi}{\partial\theta} = \sum_s d_\pi(s)\sum_a \frac{\partial\pi(a|s)}{\partial\theta} \bar{q}_\pi(s,a)
\end{align}
Is the policy gradient with which gradient ascent on the policy can be performed in order to maximize $\rho$. A proof and a thorough discussion can be found in \cite{PGBS}. Note that the policy gradient is on-policy and that $\bar{q}_\pi$ is an approximation of $q_\pi$.\\
Since $\pi$ is a probability distribution it follows $\frac{\partial\pi(a|s)}{\partial\theta}=0, \forall s \in S$. Therefore
\begin{align}
\frac{\partial\rho_\pi}{\partial\theta} = \sum_s d_\pi(s)\sum_a \frac{\partial\pi(a|s)}{\partial\theta}\left[ \bar{q}_\pi(s,a)+b(s)\right],\text{\hspace{12mm}} b:S\rightarrow \mathop{\mathbb{R}}
\end{align}
the function $b$ is called a baseline and is often used to reduce variance and bias of the gradient ascent update step. Using $\frac{\nabla_\theta\pi(a|s)}{\pi(a|s)} = \nabla_\theta ln(\pi(a|s))$ and $\mathop{\mathbb{E}}_{x\sim p(x)}[f(x)] = \sum_{x}p(x)f(x)$, rewriting eq (1.13) yields
\begin{align}
\frac{\partial\rho_\pi}{\partial\theta} = \mathop{\mathbb{E}}_{\substack{s\sim d_\pi(s) \\ a\sim \pi(a|s)}}\left[ \nabla_\theta ln (\pi(a|s))\left[ \bar{q}_\pi(s,a)+b(s)\right]\right]
\end{align}
In practice where there are large state and action spaces the expectations w.r.t $s$ and $a$ become infeasible to obtain. Using $\mathop{\mathbb{E}}_{x\sim p(x)}[f(x)] = \frac{1}{n}\sum_{n}f(x), n\rightarrow\infty, x\sim p(x)$ , ample based learning uses enough samples of $s$ and $a$ in order to obtain a good enough approximation of the expectations. Therefore eq. (1.15) becomes

\begin{align}
\frac{\partial\rho_\pi}{\partial\theta} = \frac{1}{n}\sum_{n} \left[ \nabla_\theta ln (\pi(a|s))\left[ \bar{q}_\pi(s,a)+b(s)\right]\right],\text{\hspace{8mm}} s\sim d_\pi(s), \text{\hspace{4mm}} a\sim \pi(a|s), \text{\hspace{4mm}} n\rightarrow\infty
\end{align}

This leads to Actor Critic methods (A2C) where there are two instances that are updated in a turn based fashion. The critic is the value-function approximation and the actor is approximating the policy. Intuitively the critic evaluates the action taken by the actor who uses this evaluation to scale its gradient update (this is the role of $\bar{q}_\pi$ in eq (1.13)).

\subsection{Soft Actor-Critic}
This algorithm was introduced by \cite{haarnoja2018soft}.
