\BOOKMARK [1][-]{section*.1}{Summary of Notation}{}% 1
\BOOKMARK [1][-]{section.0.1}{Introduction}{}% 2
\BOOKMARK [0][-]{chapter.1}{Motivation}{}% 3
\BOOKMARK [0][-]{chapter.2}{Preliminaries}{}% 4
\BOOKMARK [1][-]{section.2.1}{Image segmentation}{chapter.2}% 5
\BOOKMARK [1][-]{section.2.2}{Reinforcement Learning \(RL\)}{chapter.2}% 6
\BOOKMARK [2][-]{subsection.2.2.1}{Value functions}{section.2.2}% 7
\BOOKMARK [2][-]{subsection.2.2.2}{Q-learning}{section.2.2}% 8
\BOOKMARK [2][-]{subsection.2.2.3}{Policy gradient methods}{section.2.2}% 9
\BOOKMARK [2][-]{subsection.2.2.4}{Maximum Entropy Reinforcement Learning}{section.2.2}% 10
\BOOKMARK [2][-]{subsection.2.2.5}{Soft Actor-Critic \(SAC\)}{section.2.2}% 11
\BOOKMARK [2][-]{subsection.2.2.6}{Common optimization methods}{section.2.2}% 12
\BOOKMARK [2][-]{subsection.2.2.7}{Reparameterization}{section.2.2}% 13
\BOOKMARK [2][-]{subsection.2.2.8}{Normalizing flows}{section.2.2}% 14
\BOOKMARK [2][-]{subsection.2.2.9}{The Heugh transorm}{section.2.2}% 15
\BOOKMARK [1][-]{section.2.3}{Geometric deep learning}{chapter.2}% 16
\BOOKMARK [1][-]{section.2.4}{Mutex watershed}{chapter.2}% 17
\BOOKMARK [1][-]{section.2.5}{Image partitioning by multicuts}{chapter.2}% 18
\BOOKMARK [1][-]{section.2.6}{Principal component analysis}{chapter.2}% 19
\BOOKMARK [1][-]{section.2.7}{Loss functions}{chapter.2}% 20
\BOOKMARK [2][-]{subsection.2.7.1}{Dice loss}{section.2.7}% 21
\BOOKMARK [2][-]{subsection.2.7.2}{Contrasive loss}{section.2.7}% 22
\BOOKMARK [2][-]{subsection.2.7.3}{Triplet loss}{section.2.7}% 23
\BOOKMARK [0][-]{chapter.3}{Methods}{}% 24
\BOOKMARK [1][-]{section.3.1}{Using RL for the image segmentation task}{chapter.3}% 25
\BOOKMARK [1][-]{section.3.2}{Using RL for predicting affinities}{chapter.3}% 26
\BOOKMARK [1][-]{section.3.3}{Overview over the pipeline}{chapter.3}% 27
\BOOKMARK [1][-]{section.3.4}{The pipeline in the RL terminology}{chapter.3}% 28
\BOOKMARK [2][-]{subsection.3.4.1}{The state}{section.3.4}% 29
\BOOKMARK [2][-]{subsection.3.4.2}{The actions}{section.3.4}% 30
\BOOKMARK [2][-]{subsection.3.4.3}{The reward}{section.3.4}% 31
\BOOKMARK [2][-]{subsection.3.4.4}{The agent}{section.3.4}% 32
\BOOKMARK [2][-]{subsection.3.4.5}{The environment}{section.3.4}% 33
\BOOKMARK [2][-]{subsection.3.4.6}{The problem of local optima}{section.3.4}% 34
\BOOKMARK [2][-]{subsection.3.4.7}{Definition of the RL algorithm}{section.3.4}% 35
\BOOKMARK [1][-]{section.3.5}{Obtaining superpixels from mutex watershed}{chapter.3}% 36
\BOOKMARK [1][-]{section.3.6}{The embedding network}{chapter.3}% 37
\BOOKMARK [1][-]{section.3.7}{The actor critic networks}{chapter.3}% 38
\BOOKMARK [1][-]{section.3.8}{Finding subgraphs}{chapter.3}% 39
\BOOKMARK [2][-]{subsection.3.8.1}{Thoughts on dependence}{section.3.8}% 40
\BOOKMARK [1][-]{section.3.9}{Technical details}{chapter.3}% 41
\BOOKMARK [2][-]{subsection.3.9.1}{Batch processing}{section.3.9}% 42
\BOOKMARK [0][-]{chapter.4}{Experiments and results}{}% 43
\BOOKMARK [1][-]{subsection.4.0.1}{Supervised training}{chapter.4}% 44
\BOOKMARK [2][-]{subsection.4.0.2}{Unsupervised training}{subsection.4.0.1}% 45
\BOOKMARK [0][-]{chapter.5}{Conclusion}{}% 46
\BOOKMARK [1][-]{section*.16}{References}{chapter.5}% 47
